{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from eolearn.core import EOPatch, FeatureType, OverwritePermission\n",
    "from eolearn.io import ExportToTiff\n",
    "from fs_s3fs import S3FS\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage.exposure import match_histograms\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "from cv2 import INTER_CUBIC, GaussianBlur, resize\n",
    "from hrnet.src.predict import Model\n",
    "from hrnet.src.train import resize_batch_images\n",
    "from sr.data_loader import EopatchPredictionDataset, ImagesetDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sr.metrics import minshift_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wandb login <WANDB KEY>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_access_key_id = ''\n",
    "aws_secret_access_key = ''\n",
    "\n",
    "filesystem = S3FS(\n",
    "    bucket_name='',\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key, region='eu-central-1')\n",
    "\n",
    "\n",
    "# If 'LOCAL' it will be loaded from local wandb storage,  if 'WANDB' from online storage\n",
    "MODEL_LOCATION = 'LOCAL'\n",
    "\n",
    "MODEL_NAME = ''\n",
    "MODEL_PREFIX = ''\n",
    "MATCHES_S2 = True\n",
    "LOCATION = f'wandb/latest-run/files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOP_COUNTRIES_PQ = f'eop-countries_overlapped.pq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(EOP_COUNTRIES_PQ):\n",
    "    eops_countries = []\n",
    "    for eopfname in filesystem.listdir(''):\n",
    "        eop = EOPatch.load(os.path.join('',\n",
    "                           eopfname), filesystem=filesystem, lazy_loading=True)\n",
    "        eops_countries.append({'country': 'Lithuania' if str(eop.bbox.crs) == 'EPSG:32634' else 'Cyprus',\n",
    "                               'eopatch': eopfname})\n",
    "        pd.DataFrame(eops_countries).to_parquet(\n",
    "            f'eop-countries_overlapped.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filename = 'HRNet.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_LOCATION == 'WANDB':\n",
    "    model_checkpoint = wandb.restore(\n",
    "        checkpoint_filename, run_path=LOCATION, replace=True)\n",
    "    model_checkpoint = open(checkpoint_filename, 'rb')\n",
    "    model_config_yaml = yaml.load(wandb.restore(\n",
    "        'config.yaml', run_path=LOCATION, replace=True))\n",
    "elif MODEL_LOCATION == 'LOCAL':\n",
    "    model_checkpoint = os.path.join(LOCATION, checkpoint_filename)\n",
    "    model_config_yaml = yaml.load(open(os.path.join(LOCATION, 'config.yaml')))\n",
    "\n",
    "    assert os.path.isfile(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {k: v['value']\n",
    "          for k, v in model_config_yaml.items() if 'wandb' not in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_norm_df = pd.read_parquet(\n",
    "    filesystem.openbin('metadata/s2_norm_per_country.pq'))\n",
    "\n",
    "norm_deimos = {k: v for k, v in np.load(\n",
    "    filesystem.openbin('metadata/deimos_min_max_norm.npz')).items()}\n",
    "norm_s2 = {k: v for k, v in np.load(\n",
    "    filesystem.openbin('metadata/s2_min_max_norm.npz')).items()}\n",
    "\n",
    "data_df = pd.read_parquet(filesystem.openbin('metadata/npz_info_small.pq'))\n",
    "data_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.read_parquet(filesystem.openbin('scores-bicubic-32x32.pq')).rename(columns={'name': 'singleton_npz_filename'})\n",
    "data_df = pd.merge(data_df, scores_df, on='singleton_npz_filename')\n",
    "data_df['MSE_ratio'] = data_df['MSE_s']/data_df['MSE_s_c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = data_df[(data_df['SSIM_s_c'] > .2) &\n",
    "                        (data_df['PSNR_s_c'] > 10) &\n",
    "                        (data_df['MSE_ratio'] < 10) &\n",
    "                        (data_df['is_shadow_v2'] == False) &\n",
    "                        (data_df['countries'] == 'Lithuania') &\n",
    "                        (data_df['num_tstamps'] > 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(config)\n",
    "model.load_checkpoint(checkpoint_file=model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = filtered_data[(filtered_data.train_test_validation == 'validation')].sample(\n",
    "    2000).singleton_npz_filename.values\n",
    "\n",
    "test_dataset = ImagesetDataset(\n",
    "    imset_dir=config['paths']['prefix'],\n",
    "    imset_npz_files=test_samples,\n",
    "    country_norm_df=country_norm_df,\n",
    "    normalize=True,\n",
    "    norm_deimos_npz=norm_deimos,\n",
    "    norm_s2_npz=norm_s2,\n",
    "    channels_labels=config['training']['channels_labels'],\n",
    "    channels_feats=config['training']['channels_features'],\n",
    "    time_first=True,\n",
    "    n_views=config['training']['n_views'],\n",
    "    histogram_matching=config['training']['histogram_matching']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_bands(eop, bands_name, eop_name, norm_df):\n",
    "    \"\"\" Normalise bands \"\"\"\n",
    "    df_means = norm_df[norm_df.eopatch == eop_name].groupby('month').mean()[cols_mean]\n",
    "    df_std = norm_df[norm_df.eopatch == eop_name].groupby('month').mean()[cols_std]\n",
    "    \n",
    "    bands = eop.data[bands_name]\n",
    "    \n",
    "    normalised = np.empty(bands.shape, dtype=np.float32)\n",
    "    \n",
    "    for nb, (band, ts) in enumerate(zip(bands, eop.timestamp)):\n",
    "        means = df_means.loc[ts.strftime('%Y-%m')].values\n",
    "        stds = df_std.loc[ts.strftime('%Y-%m')].values\n",
    "        \n",
    "        normalised[nb] = (band - means) / stds\n",
    "        \n",
    "    return normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr = np.moveaxis(sample['hr'].numpy(), 0, 2)\n",
    "\n",
    "hr_ = resize(GaussianBlur(hr, ksize=(7, 7), sigmaX=4), None, fx=1/4, fy=1/4)\n",
    "\n",
    "hr__ = resize(hr_, None, fx=4, fy=4, interpolation=INTER_CUBIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = np.moveaxis(\n",
    "    sample['lr'][np.sum(sample['alphas'].int().numpy())-1].numpy(), 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=4, figsize=(15, 7.5))\n",
    "axs[0].imshow(hr[..., [2, 1, 0]])\n",
    "axs[1].imshow(hr_[..., [2, 1, 0]])\n",
    "axs[2].imshow(lr[..., [2, 1, 0]])\n",
    "axs[3].imshow(hr__[..., [2, 1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_ = match_histograms(lr, hr_, multichannel=True)\n",
    "lr__ = resize(lr_, None, fx=4, fy=4, interpolation=INTER_CUBIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=4, figsize=(15, 7.5))\n",
    "axs[0].imshow(hr[..., [2, 1, 0]])\n",
    "axs[1].imshow(hr_[..., [2, 1, 0]])\n",
    "axs[2].imshow(lr_[..., [2, 1, 0]])\n",
    "axs[3].imshow(lr__[..., [2, 1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssims_bi_de, psnrs_bi_de = [], []\n",
    "ssims_bi_s2, psnrs_bi_s2 = [], []\n",
    "ssims_sr, psnrs_sr = [], []\n",
    "\n",
    "for sample in tqdm(test_dataloader):\n",
    "    sr = torch.from_numpy(model(sample))\n",
    "    alphas = sample['alphas'].float()\n",
    "    lrs = sample['lr'][np.arange(len(alphas)),\n",
    "                        torch.sum(alphas, dim=1, dtype=torch.int64) - 1]\n",
    "    hr = sample['hr'].float()\n",
    "\n",
    "    lrs_hm = torch.tensor([match_histograms(np.moveaxis(lri.numpy(), 0, 2),\n",
    "                                             np.moveaxis(hri.numpy(), 0, 2),\n",
    "                                             multichannel=True)\n",
    "                            for (lri, hri) in zip(lrs, hr)])\n",
    "    \n",
    "    lrs_hm = lrs_hm.permute([0, 3, 1, 2])\n",
    "\n",
    "    baseline_s2 = resize_batch_images(lrs_hm, fx=4, fy=4).float()\n",
    "\n",
    "    baseline_de = torch.tensor([resize(resize(GaussianBlur(np.moveaxis(hr_.numpy(), 0, 2),\n",
    "                                            ksize=(7, 7),\n",
    "                                            sigmaX=4), None, fx=1/4, fy=1/4),\n",
    "                       None, fx=4, fy=4, interpolation=INTER_CUBIC) for hr_ in hr])\n",
    "    baseline_de = baseline_de.permute([0, 3, 1, 2])\n",
    "\n",
    "    ssims_sr.append(minshift_loss(hr, sr, metric='SSIM', apply_correction=False)[0])\n",
    "    ssims_bi_de.append(minshift_loss(hr, baseline_de, metric='SSIM', apply_correction=False)[0])\n",
    "    ssims_bi_s2.append(minshift_loss(hr, baseline_s2, metric='SSIM', apply_correction=False)[0])\n",
    "\n",
    "    psnrs_sr.append(minshift_loss(hr, sr, metric='PSNR', apply_correction=False)[0])\n",
    "    psnrs_bi_de.append(minshift_loss(hr, baseline_de, metric='PSNR', apply_correction=False)[0])\n",
    "    psnrs_bi_s2.append(minshift_loss(hr, baseline_s2, metric='PSNR', apply_correction=False)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim_bi_de = np.array([jj for item in ssims_bi_de for jj in item.numpy()])\n",
    "ssim_bi_s2 = np.array([jj for item in ssims_bi_s2 for jj in item.numpy()])\n",
    "ssim_sr = np.array([jj for item in ssims_sr for jj in item.numpy()])\n",
    "\n",
    "psnr_bi_de = np.array([jj for item in psnrs_bi_de for jj in item.numpy()])\n",
    "psnr_bi_s2 = np.array([jj for item in psnrs_bi_s2 for jj in item.numpy()])\n",
    "psnr_sr = np.array([jj for item in psnrs_sr for jj in item.numpy()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}